defaults:
  - model: d12
  - optimizer: soap
  - data: fineweb
  - _self_
  - experiment: ???

# Training configuration
training:
  batch_size: 48
  accumulation: 2
  sequence_length: 1024
  num_iterations: 1500
  learning_rate: 0.0018
  warmup_iters: 250
  warmdown_iters: 2000
  weight_decay: 0.5

# Evaluation configuration
evaluation:
  val_loss_every: 128
  val_max_steps: 20
  save_every: 0

# Distributed training
distributed:
  backend: nccl
  find_unused_parameters: false

# Logging
logging:
  log_dir: logs
  log_every: 1

# Hardware
hardware:
  device: cuda
  dtype: bfloat16
  compile: true 

experiment_name: baseline