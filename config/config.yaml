defaults:
  - model: d12
  - optimizer: soap
  - data: fineweb
  - _self_
  - experiment: ???

# Training configuration
training:
  batch_size: 48
  accumulation: 2
  sequence_length: 1024
  num_iterations: 1500
  learning_rate: 0.0018
  warmup_iters: 250
  warmdown_iters: 0
  weight_decay: 0.5

# Evaluation configuration
evaluation:
  val_loss_every: 128
  val_max_steps: 20
  save_every: 0
  # Text sampling configuration
  sample_every: 512  # How often to generate text samples (defaults to val_loss_every)
  num_unconditional_samples: 5  # Number of unconditional text samples
  num_completion_samples: 5  # Number of completion samples from validation set
  max_new_tokens: 128  # Maximum tokens to generate per sample
  temperature: 1.0  # Sampling temperature
  top_k: 50  # Top-k sampling parameter
  prompt_length: 32  # Length of prompt for completion samples
  sample_seed: 42  # Seed for reproducible text sampling
  debug_sampling: false  # Print debug info for sampling consistency

# Distributed training
distributed:
  backend: nccl
  find_unused_parameters: false

# Logging
logging:
  log_dir: logs
  log_every: 1

# Hardware
hardware:
  device: cuda
  dtype: bfloat16
  compile: true 

# muP config
mup:
  enabled: false
  cfg:
    disable_attention_scaling: false # Uses 1/sqrt(d_head) attn scaling instead of 1/d_head
    disable_hidden_lr_scaling: false # Disables muP hidden LR adjustment
    width_multiplier: 1.0 # mup_width_multiplier = width / base_width
    input_alpha: 1.0 # Optional tunable multiplier applied to input embedding forward pass output
    output_alpha: 1.0 # Optional tunable multiplier applied to output unembedding forward pass output
    enable_coord_check_logging: false # If True will track the output.abs().mean() of various layers throughout training
    init_std: 0.02 # Initialization std for muP

experiment_name: baseline